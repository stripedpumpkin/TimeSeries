{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to go through the ARIMA model to evaluate its performance in a univariate dataset. Also,its performance will be compared with other techniques that are currently available to create predictions in time series using neural networks. \n",
    "\n",
    "The dataset that is going to be used, belongs to the UCI repository (http://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data)\n",
    "\n",
    "It has an hourly measurement of the quantity of PM2.5 in the air in the city of Beijing. This dataset comes with other information such as the meteorological data from that city. However, as the purpose of this exercise is to make a model from a univariate variable, this other information is not going to be taken into account. \n",
    "We will go step by step through the whole process: starting by importing the data, getting some insights to it, applying the ARIMA model and finally comparing the results with a neural network to evaluate the performance of each model.\n",
    "\n",
    "(Disclosure)\n",
    "This post consists of different methods for forecasting time series. However, none of these methods are perfect as there is no perfect way to predict the future, so these results should be taken with care and always with the advice of an expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import itertools\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('PRSA_data_2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for the date\n",
    "dataset['Timestamp'] = dataset['year'].map(str) + \"/\" + dataset['month'].map(str) + \"/\" + dataset['day'].map(str) + ' ' + dataset['hour'].map(str) + \":\" + \"00:00\"\n",
    "dataset['Timestamp'] = pd.to_datetime(dataset['Timestamp'])\n",
    "\n",
    "dataset = dataset.set_index('Timestamp')\n",
    "\n",
    "# Drop the columns that are not going to be used\n",
    "dataset = dataset.drop(['No',\n",
    " 'year',\n",
    " 'month',\n",
    " 'day',\n",
    " 'hour',\n",
    " 'DEWP',\n",
    " 'TEMP',\n",
    " 'PRES',\n",
    " 'cbwd',\n",
    " 'Iws',\n",
    " 'Is',\n",
    " 'Ir'], axis=1)\n",
    "\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned previously this dataset contains the information about the amount of PM2.5 (ug/m^3) concentration of particles in the air of Beijing. \n",
    "\n",
    "These are particles with a diameter of less than 2.5 micrometers that are floating in the air.\n",
    "(https://www.who.int/news-room/fact-sheets/detail/ambient-(outdoor)-air-quality-and-health)\n",
    "The world health organization states that exposure to these particles can cause cardiovascular and respiratory diseases and cancer. They estimate that this air pollution caused 4.2 million premature death per year in 2016. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the data to see what we need to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "register_matplotlib_converters()\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(dataset.index, dataset['pm2.5'])\n",
    "plt.title(\"Levels of PM2.5 particles in Beijing\")\n",
    "plt.ylabel(\"PM2.5 concentration (ug/m^3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding this first plot, we can see that at the end and at the beginning of each year the concentration of PM2.5 particles is  higher than in other periods of the year, but it is still fuzzy data and we can barely have a better understanding of what is really happening. Because of this some transformations are needed to be able get a deeper insight into our data. There are many tools available, and here we are only going to use two, but there are many others that can provide better or different insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by different frames of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is as simple as to compute the mean of the data measurements by week or month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = dataset['pm2.5'].resample('W').mean()\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(weekly)\n",
    "plt.title(\"The weekly average of PM2.5 concentration\")\n",
    "plt.ylabel(\"PM2.5 particles\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = dataset['pm2.5'].resample('M').mean()\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(monthly)\n",
    "plt.title(\"The monthly average of PM2.5 concentration\")\n",
    "plt.ylabel(\"PM2.5 particles\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plots, the information that we get from the data using this method is not very clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are still going to use the mean but in a different way, by using the Moving Average, that computes the average of N given time steps. It will smooth the data allowing the viewer to infer some visible patterns or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movingAverage = dataset['pm2.5'].rolling(window=24).mean()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Moving average\\n Window size 24, equivalent to one day\")\n",
    "plt.ylabel(\"PM2.5 particles\")\n",
    "plt.plot(movingAverage,label=\"Rolling mean trend\")\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movingAverage = dataset['pm2.5'].rolling(window=(24*7)).mean()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Moving average\\n Window size 24*7\")\n",
    "plt.ylabel(\"PM2.5 particles\")\n",
    "plt.plot(movingAverage,label=\"Rolling mean trend\")\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movingAverage = dataset['pm2.5'].rolling(window=(24*7*30)).mean()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Moving average\\n Window size 5.040, equivalent to one month\")\n",
    "plt.plot(movingAverage,label=\"Rolling mean trend\")\n",
    "plt.ylabel(\"PM2.5 particles\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last plot, we can clearly spot some regular patterns. However, these methods are very sensitive to outliers and as we can see in the first plot our data has many of them. \n",
    "\n",
    "One can think of these outliers as faulty measures by the devices related to any kind of misfunction. However, doing some research we will find that these levels have been already reached and documented in the capital of China.\n",
    "\n",
    "https://www.theguardian.com/world/2013/jan/13/beijing-breathe-pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting closer to the last weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we have evaluated so far looks quite messy, and as the objective of this small project is to apply different forecasting techniques, we are going to focus the efforts in the last four weeks of the entire dataset. By doing so the visualizations are going the be easier and we will see clearly how the predictions fit in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_weeks = dataset.loc['2014-12':'2014']\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"PM2.5 particules in December 2014\")\n",
    "plt.ylabel(\"PM2.5 particles\")\n",
    "plt.plot(last_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(last_weeks['pm2.5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the distribution of the data, we can see that most of the data is grouped in the first values, looking like the exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_weeks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data dataset, there are 716 measures of the particle's concentration, around one month of data. With a mean of 78 particles per hour. This means is considered as unhealthy and people should not the exposed during long periods of time to these levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying any statistical model it's important to check if our data is considered as stationary\n",
    "\n",
    "Stationarity basically means that the properties such as the mean and variance don't change over time. \n",
    "(https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm)\n",
    "\n",
    "There are various ways to check if data is stationary, and one good way is the Test Dickey-Fuller, which states that if the p-value is lower than a given threshold it won't be considered as stationary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dickey-Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Perform Dickey-Fuller test:\n",
    "print ('Results of Dickey-Fuller Test:')\n",
    "dftest = adfuller(last_weeks['pm2.5'], autolag='AIC')\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print (dfoutput)\n",
    "\n",
    "p_value = dftest[1]\n",
    "\n",
    "if p_value <= 0.01:\n",
    "    print(\"\\nData is stationary\")\n",
    "else:\n",
    "    print(\"\\n Data is non-stationary \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we proved with Dickey-Fuller Test, fortunately, our data is stationary which means that it doesn't have any kind of trend and thereforeit doesn't need any transformations and we can directly apply ARIMA.\n",
    "\n",
    "For the cases the time series is non-stationary there are some transformations that can be applied to make it stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going through ARIMA we are going to split the data that will help us to train the model, and after that evaluate how accurate it is with the test dataset.\n",
    "\n",
    "Train dataset has the data of 29 days and the test set has 2 days.\n",
    "\n",
    "The purpose of splitting the dataset is because the model has to be tested with some labelled data, that means to see how the predictions are close to the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = last_weeks['2014-12': '2014-12-29']\n",
    "test_dataset = last_weeks['2014-12-30': '2014']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA stands for Autorregresive Integrated Moving Average. It is used for time series forecasting.\n",
    "\n",
    "It contains three different components. The autoregressive the regression of the time series onto himself, the Integrated (I) component is to correct the non-stationarity of the data. The last component Moving Average (MA) models the errors based on past errors.\n",
    "\n",
    "Each component receives different parameters AR(p), I(d), MA(q). To estimate the value of each parameter we need to get the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n",
    "\n",
    "This functions will tell us how correlated are the observations in the time series. \n",
    "\n",
    "Also, and the main purpose is that the will indicate us which are the best coefficient to use in our ARIMA model.\n",
    "\n",
    "\n",
    "In our case the pattern in the PACF (significant correlations at the first or second lag followed by correlations that are not significant). The number of significant correlations in the PACF tells us the term of the autoregressive (AR) model. In our case, the coefficient is three.\n",
    "\n",
    "\n",
    "However, based on the pattern of the ACF (the function progressively decrease), we cannot infer the term for the Moving Average (MA) so the best option is to use zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(train_dataset['pm2.5'], lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(train_dataset['pm2.5'], lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply ARIMA to our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(train_dataset, order=(3,0,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first plot shows the residuals of our data, and we can observe that most of the data is distributed around zero. Let's see more in detail how this is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.plot(kind='kde')\n",
    "plt.xlim([-100.0, 100.0])\n",
    "plt.show()\n",
    "\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a similar shape as the Normal or Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some predictions\n",
    "\n",
    "Now it is time to get some predictions from the model, to evaluate how accurate our model is, we are going to use the test dataset we have made.\n",
    "Also, we need to perform a rolling forecast, that means after calculating each prediction add it and recalculate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = test_dataset.values\n",
    "\n",
    "X = train_dataset.values\n",
    "size = len(X)\n",
    "history = [x for x in X]\n",
    "\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "\tmodel = ARIMA(history, order=(3,0,0))\n",
    "\tmodel_fit = model.fit(disp=0)\n",
    "\toutput = model_fit.forecast()\n",
    "\tyhat = output[0]\n",
    "\tpredictions.append(yhat)\n",
    "\tobs = test[t]\n",
    "\thistory.append(obs)\n",
    "\tprint('predicted=%f, expected=%f' % (yhat, obs))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(test, label='Test')\n",
    "plt.plot(predictions, label='Predictions', color='red')\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = error\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above we can see the result of our data in the test data, looking like pretty well fitted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "Calculate the Mean Squared Error to obtain the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(test, predictions)\n",
    "print('Test Mean Squared Error(MSE): %.3f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Now a model of neural networks is going to be applied, concretely we are going to use Recurrent Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is a type of neural networks, which mainly are used in natural language processing (NLP) and predicting Time Series. The basic idea behind these type is that the state of each neuron is stored and fed to the next layer. This is why it is so used in NLP and Time series because it has in count the last observations to predict the next one.\n",
    "\n",
    "There is a type of RNN called Long Short-Term Memory, it was proposed to solve a huge problem that all the RNN presented, this is called The Vanishing Gradient Problems but we are not going to enter in this kind of details. This is all we need to know to apply to our dataset.\n",
    "\n",
    "This part is going to be much shorter than the previous one because basically to apply Neural Networks is easier than to apply ARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the example, we are going to work on the last four weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_weeks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the datasets to train and test the network are going to be the same to correctly compare the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = last_weeks['2014-12': '2014-12-29']\n",
    "test_dataset = last_weeks['2014-12-30': '2014']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better accuracy in our network we need to first normalize the values of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalize = MinMaxScaler(feature_range = (0, 1))\n",
    "train_dataset = normalize.fit_transform(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is ranged between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recurrent Neural Networks predict the new observation based on X lagged observations. For our case, lets try with 168 lagged observations what means around one week to get a new prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(168, train_dataset.shape[0]):\n",
    "    X_train.append(train_dataset[i-168:i, 0])\n",
    "    y_train.append(train_dataset[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create the architecture of the Neural Network! We are going to use Keras, which is a popular framework that is made on top of TensorFlow (the neural network library published by Google). Keras is so famous because it makes the task to build the Networks really easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture of the network\n",
    "\n",
    "Our network is made by LSTM layers, it contains five networks with 100x200x300x200x100 networks.\n",
    "I used dropout to improve accuracy and reduce the loss after epoch.\n",
    "In order to calculate the loss of the network, I used the mean squared error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 200, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 300, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 200, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Adding a fiveth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is fitted to our train dataset the model and it's time to create the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = last_weeks[len(last_weeks) - len(test_dataset) - 168 :].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = normalize.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(168, len(inputs)):\n",
    "    X_test.append(inputs[i-168:i, 0])\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted = regressor.predict(X_test)\n",
    "predicted = normalize.inverse_transform(predicted)\n",
    "\n",
    "# Visualising the results\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(test_dataset['pm2.5'].values, color = 'red', label = 'Real Values')\n",
    "plt.plot(predicted, color = 'blue', label = 'Predicted Values')\n",
    "plt.title('Contamination prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks that the predictions suit well to the real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(test_dataset['pm2.5'].values, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw many different things starting by different concepts of statistical model and time series manipulations (different transformations, stationarity in time series, autocorrelation functions). Following by one statistical model (ARIMA) and Recurrent Neural Networks applied to our particular data set.\n",
    "\n",
    "We can see that ARIMA performs better to our test set with a smaller Mean squared error of 338.96 compared to 542 provided by the neural networks.\n",
    "\n",
    "There are still some other things to try but for this specific dataset with around 720 observations the price goes to the statistical model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is understood the first thing that comes to mind is to apply the same methods to the whole dataset (with more than 41.000 observations. Here we will see which one behave better with a higher amount of data.\n",
    "\n",
    "The next (still with the same dataset) is to use the other variables to create the forecasting. In the beginning, we have got rid of around 7 different variables with valuable information that we are not using in our model. \n",
    "\n",
    "We can also, improve the architecture of the neural network until we reach an optimal point.\n",
    "There is another statistical model such as Seasonal Arima (SARIMA) that can maybe improve the metrics.\n",
    "\n",
    "Finally and the purpose of all of this study is to extract useful insights from the data, in our case, it will be about the PM2.5 levels in Beijing.\n",
    "\n",
    "... but let see all these things in new posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\n",
    "\n",
    "https://towardsdatascience.com/forecasting-exchange-rates-using-arima-in-python-f032f313fc56\n",
    "\n",
    "https://www.kaggle.com/berhag/co2-emission-forecast-with-python-seasonal-arima/data\n",
    "\n",
    "https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
